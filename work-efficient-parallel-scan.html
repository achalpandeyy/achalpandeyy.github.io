<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Work-efficient Parallel Scan</title>
    <meta name="description" content="">
    <link rel="stylesheet" href="styles.css">
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
        });
    });
    </script>
</head>
<body>
    <h1 class="center-text">Work-efficient Parallel Scan</h1>
    <p class="center-text">
        <a href="./writing.html">← Back to Writing</a>
    </p>
    <hr>
    <h1>Work-efficiency</h1>
    <p>Less work efficiency means that we are doing more operations than we need to do in order to solve the problem. So coming up with a more work-efficient algorithm will fundamentally be about finding ways to reduce the number of operations.</p>
    <p>This is exactly what we did when trying to save operations in the naive algorithm to come up the Kogge-Stone — we found ways to reuse the partial sums. Can we ride this train even further? The answer is yes.</p>
    <div style="text-align: center;">
        <img src="images/kogge_stone.png" alt="Kogge-Stone computation diagram" style="width: 100%; max-width: 600px;">
    </div>
    <p>Just by looking at this computation for Kogge-Stone, it is not intutively clear to me that we can.</p>
    <p>Instead of numbers let's switch to using variables because it will be easier to track which values ended up where through various levels of the tree. The syntax $[x_i \space .. \space x_j]$ represents sum all of numbers starting at $x_i$ and ending at $x_j$, both inclusive.</p>
    <p>
    Look at the evolution of $x_2$ and $x_3$:
    <ul>
        <li>
            $x_2 \rightarrow (x_1 + x_2) \rightarrow x_0 + (x_1 + x_2)$
        </li>
        <li>
            $x_3 \rightarrow (x_2 + x_3) \rightarrow (x_0 + x_1) + (x_2 + x_3)$
        </li>
    </ul>
    That is four total additions. One on each level for both $x_2$ and $x_3$.
    </p>
    <p>
    None of the work we did through the evolution of $x_2$ was directly usable for $x_3$ even though the operations are happening on the same four elements: $x_0$, $x_1$, $x_2$ and $x_3$ — three out of four elements are the same.
    </p>
    <p>
        Contrast this with the serial algorithm:
        <ul>
            <li>
                $x_2 \rightarrow (x_0 + x_1) + x_2$
            </li>
            <li>
                $x_3 \rightarrow (x_0 + x_1 + x_2) + x_3$
            </li>
        </ul>
        Both of them needs only one more addition to the previous value, making only two total additions (50% less). Also, the entire value we compute in the previous step is reusable — efficient!
    </p>
    <h1>Upward phase</h1>
    <p>We know that we need to compute a reduction of a subarray starting at 0 and ending at the current index, for every element. We also know that a reduction tree is the most work-efficient way of computing this reduction. So we start by computing the reduction of the entire array. We know that this will at least give us the scan result for the final element.</p>
    <p>But look at that! We also got the results for positions 0, 1 and 3, basically for free. By now we have done only $N-1$ additions, which is the same as the serial version.</p>
    <p>The upward phase is just one reduction tree and we know that it requires only $N - 1$ additions so the work complexity is linear i.e. $O(N)$.</p>
    <pre><code class="language-cpp">for (int stride = 1; stride <= block_dim; stride *= 2)
{
    int index = (threadIdx.x + 1)*2*stride - 1;
    if (index < 2*block_dim)
        segment[index] += segment[index - stride];
    __syncthreads();
}</code></pre>
    <h1>Downward phase</h1>
    <p>There are two communication patterns for the downward phase based on the two different algorithms. One of them produces natively inclusive results while the other produces natively exclusive.</p>
    <h2>Brent-Kung (natively inclusive)</h2>
    <p>
        Let's inspect the array and see what we need for the other elements to get to their scanned results.
        <ul>
            <li>$0: x_0 \rightarrow done$</li>
            <li>$1: x_0 + x_1 \rightarrow done$</li>
            <li>$2: x_2 \rightarrow \space ? \rightarrow [x_0 \space .. \space x_2]$</li>
            <li>$3: x_0 + x_1 + x_2 + x_3 \rightarrow done$</li>
            <li>$4: x_4 \rightarrow \space ? \rightarrow [x_0 \space .. \space x_4]$</li>
            <li>$5: x_4 + x_5 \rightarrow \space ? \rightarrow [x_0 \space .. \space x_5]$</li>
            <li>$6: x_6 \rightarrow \space ? \rightarrow [x_0 \space .. \space x_6]$</li>
            <li>$7: x_7 \rightarrow [x_0 \space .. \space x_7]$</li>
        </ul>
        We still have incomplete results for positions: 2, 4, 5, and 6. How do we complete those? Let's see.
        <ul>
            <li>$x_2$ needs $[x_0 \space .. \space x_1]$</li>
            <li>$x_4$ needs $[x_0 \space .. \space x_3]$</li>
            <li>$x_5$ needs $[x_0 \space .. \space x_3]$</li>
            <li>$x_6$ needs $[x_0 \space .. \space x_5]$</li>
        </ul>
    </p>
    <p>
        Do we have these partial results sitting around in our array right now? The answer is yes — we have all of them. For position 2, we have the sum from $x_0$ to $x_1$. For positions 4 and 5, we have the sum from $x_0$ to $x_3$. For position 6, we don't have the sum from $x_0$ to $x_5$ directly but we can see a way of getting there, that is by combining the sums from $x_0$ to $x_3$ and from $x_4$ to $x_5$.
    </p>
    <p>
        Now all that we need to do is add the partial results we have identified to positions that need them. But now the question is: how do we organize this computation?
        Let's separate out these pairings by the distance between the partial result and the position that needs it.
        <br>
        For distance = 2, we have:
        <ul>
            <li>$5^{th}$ position needing the sum from $x_0$ to $x_3$</li>
        </ul>
        For distance = 1, we have:
        <ul>
            <li>$2^{nd}$ position needing the sum from $x_0$ to $x_1$</li>
            <li>$4^{th}$ position needing the sum from $x_0$ to $x_3$</li>
            <li>$6^{th}$ position needing the sum from $x_0$ to $x_5$</li>
        </ul>
        Now we just do the operations that require the same distance at the same level, in a tree-like fashion like we have been doing all this time.
    </p>
    <pre><code class="language-cpp">for (int stride = ((2*block_dim)/4); stride >= 1; stride /= 2)
{
    int index = (threadIdx.x + 1)*2*stride - 1;
    if (index + stride < 2*block_dim)
        segment[index + stride] += segment[index];
    __syncthreads();
}</code></pre>
    <h3>Work complexity</h3>
    <p>We have one addition in the first level and we have three in the next level. If we continue this pattern we get: 1, 3, 7, 15, .. i.e. $2^r -1, \space r >=1$.</p>
    <p>To find out the number of levels we notice that for $N = 8$ we had 2 and then 1 stride, generalizing this pattern: $N/4 \rightarrow N/8 \rightarrow N/16 \space ..$, since the last level must have stride = 1 we will have $log_2 N - 1$ number of levels.$$\therefore \space total \space additions = \sum_{r=1}^{r=k} 2^r - 1 = N - 1 - log_2N$$</p>
    <p>There is another cool way to look at this analysis. Imagine a binary tree such that every node is the sum of its children and leaf nodes are elements of the array. There will be $N - 1$ internal, that is non-leaf, nodes in this binary tree and the leftmost branch is the only one whose nodes does not need a correction to its prefix sums because they always start at 0. This makes the total additions $(N - 1) - log_2 N$ i.e. $O(N)$.</p>
    <h2>Blelloch (natively exclusive)</h2>
    <p>I don't have a good intuition for the Blelloch downward phase.</p>
    <p>
        We start by setting the last element to 0. Then we define a downsweep operator as follows:
        $$\begin{aligned}left' &\rightarrow right\\right' &\rightarrow left + right\end{aligned}$$
    </p>
    <p>After that we just apply this operator in a pattern that is the mirror image of the reduction tree computation pattern.</p>
    <p>Blelloch's downward phase will do one extra pass and more operations than Brent-Kung.</p>
    <pre><code class="language-cpp">// Save the sum before zeroing it, if you want to convert this result to an inclusive scan later.
if (threadIdx.x == blockDim.x - 1)
{
    block[threadIdx.x] = T(0);
}
__syncthreads();

for (int stride = block_dim; stride >= 1; stride /= 2)
{
    int index = (threadIdx.x + 1)*2*stride - 1;
    if (index < 2*block_dim)
    {
        T l = block[index - stride];
        T r = block[index];
        block[index] = l + r;
        block[index - stride] = r;
    }
    __syncthreads();
}</code></pre>
    <h3>Work complexity</h3>
    <p>The work complexity analysis here is straightforward. Since we do one addition for every two elements in a pattern that is the mirror image of the reduction tree pattern, we get the same number of additions i.e. $N-1$. This makes the total number of additions for Blelloch $2N - 2$ so the work complexity is $O(N)$.</p>
    <h1>Thread coarsening</h1>
    <p>Thread coarsening allows us to improve the work complexity even further because we are taking parts of parallel algorithm which is less work efficient and doing them serially which is more work efficient.</p>
    <p>Thread coarsening breaks down the block scan into three parts.</p>
    <h2>1. Sequential scan</h2>
    <p>This step sequentially scans contiguous elements assigned to a thread.</p>
    <p>Both Blelloch and Brent-Kung (unlike Kogge-Stone) can natively handle two elements per thread. So number of elements per thread is already greater than 1 even <i>before</i> we introduce a coarse factor.</p>
    <p>When we <i>actually</i> introduce a coarse factor, every thread will process twice the coarse factor number of elements. Each set of coarse factor elements must produce one result, so that, by the end of this stage, we have two elements per thread as required by both the Blelloch and Brent-Kung computation pattterns.</p>
    <pre><code class="language-cpp">// step I: thread local sequential scan
{
    u32 thread_start = (threadIdx.x*2)*coarse_factor;
    for (u32 i = 0; i < 2; ++i)
    {
        u32 start = thread_start + i*coarse_factor;
        for (u32 j = 1; j < coarse_factor; ++j)
            segment[start + j] += segment[start + (j-1)];
    }
}
__syncthreads();</code></pre>
    <p>This will always be an inclusive scan regardless of the entire scan being inclusive or exclusive.</p>
    <h2>2. Strided parallel scan</h2>
    <p>This step just applies the previously described algorithms — a reduction tree followed by a downward phase, with a stride of coarse factor between elements.</p>
    <p>We are operating on the last element of every coarse factor set of elements i.e. the thread-to-data index mapping will look like this:</p>
<pre><code>index =
    // elements for this thread start here
    (threadIdx.x*2*coarse_factor) +
    // elements for ith set of coarse_factor start here
    (i*coarse_factor) +
    // last element in the set 
    (coarse_factor - 1)</code></pre>
    <p>However, the math works out nicely and instead of applying this transform to get every data index we can simply multiply <code>stride</code> by <code>coarse_factor</code> as follows and basically preserve the structure of the non-coarsened code:</p>
    <pre><code class="language-cpp">// step II.I: reduction tree
for (int stride = 1*coarse_factor; stride <= block_dim*coarse_factor; stride *= 2)
{
    int index = (threadIdx.x + 1)*2*stride - 1;
    if (index < 2*block_dim*coarse_factor)
        s_block[index] += s_block[index - stride];
    __syncthreads();
}

// step II.II: downward (Brent-Kung)
for (int stride = ((2*block_dim)/4)*coarse_factor; stride >= 1*coarse_factor; stride /= 2)
{
    int index = (threadIdx.x + 1)*2*stride - 1;
    if (index + stride < 2*block_dim*coarse_factor)
        s_block[index + stride] += s_block[index];
    __syncthreads();
}

// step II.II downward (Blelloch)
for (u32 stride = block_dim*coarse_factor; stride >= 1*coarse_factor; stride /= 2)
{
    u32 index = (threadIdx.x + 1)*2*stride - 1;
    if (index < 2*block_dim)
    {
        T l = segment[index - stride];
        T r = segment[index];
        segment[index] = l + r;
        segment[index - stride] = r;
    }
    __syncthreads();
}</code></pre>
    <h2>3. Fixup</h2>
    <p>The operations we do in this step will be different depending on if the previous step produced an inclusive result (Brent-Kung) or an exclusive result (Blelloch).</p>
    <p>If the previous step produced an inclusive result we will just fix the first <code>coarse_factor - 1</code> elements by adding the sum of the previous <code>coarse_factor</code> set of elements to each of them.</p>
<pre><code class="language-cpp">// step III: fixup
{
    u32 thread_start = (threadIdx.x*2)*coarse_factor;
    for (u32 i = 0; i < 2; ++i)
    {
        u32 start = thread_start + i*coarse_factor;
        if (start > 0)
        {
            T prev = segment[start - 1];
            for (u32 j = 0; j < coarse_factor - 1; ++j)
                segment[start + j] += prev;
        }
    }
}
__syncthreads();</code></pre>
    <p>Whereas if the previous step produced an exclusive result we will shift the elements by 1 towards the right and add the last element to each of them.</p>
<pre><code class="language-cpp">// step III: fixup
{
    u32 thread_start = (threadIdx.x*2)*coarse_factor;
    for (u32 i = 0; i < 2; ++i)
    {
        u32 start = thread_start + i*coarse_factor;
        T last = segment[start + (coarse_factor - 1)];
        for (s32 j = coarse_factor - 1; j >= 0; --j)
        {
            T prev = 0;
            if (j > 0)
                prev = segment[start + (j - 1)];
            segment[start + j] = prev + last;
        }
    }
}
__syncthreads();</code></pre>
    <p>While both algorithms have this property that every thread can process two elements, the memory operations needed to prepare the data does not.</p>
    <p>So for moving data to and from global memory we can treat all elements per thread as one big chunk.</p>
<pre><code class="language-cpp">u32 elements_per_thread = 2*coarse_factor;
for (int i = 0; i < elements_per_thread; ++i)
{
    int smem_index = i*block_dim + threadIdx.x;
    int gmem_index = blockIdx.x*block_dim*elements_per_thread + smem_index;
    if (gmem_index < count)
        array[gmem_index] = segment[smem_index];
}</code></pre>
    <h1>Kernel configuration</h1>
    <p>Since the reduction tree increases <code>stride</code> in powers of two it can never work on non-power-of-two input elements. To solve this, we launch blocks with power-of-two dimensions. This way if the number of input elements is not divisible by the block dimensions we will pad up the input with 0s when loading in shared memory. Thus the reduction tree will always have power-of-two elements to deal with.</p>
    <p>The other constraint on block dimensions is that it must be a multiple of <code>maxThreadsPerMultiProcessor</code> otherwise we will be leaving some threads inactive. On my machine the aforementioned value is 1536 so I chose 512 as the block dimension.</p>
    <p>For smaller block dimensions we might also start running into the <code>maxBlocksPerMultiProcessor</code> limit, but block dimension of 512 is more than enough to avoid this.</p>
    <p>In theory we are also bottlenecked by the available shared memory per block but I have found it hard to hit this limit because increasing the block dimension beyond 512 is not an option and, increasing the <code>coarse_factor</code> does more harm than good, presumably because the amount of serial computation starts to become a bottleneck. Double-buffering Kogge-Stone (and even Brent-Kung, if we can) might be a good idea because we have some more shared memory that we can trade in to reduce sync overheads.</p>
    <p>Power constraints should dictate our choice of the algorithm as well — Kogge-Stone, since it is doing more operations, consumes more power than the work-efficient ones.</p>
</body>
</html>
